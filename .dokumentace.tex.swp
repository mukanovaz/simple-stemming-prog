\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{titlesec}
\usepackage{graphicx}



\begin{document}

\begin{titlepage}
        \vspace*{-2cm}
        {\centering\includegraphics[scale=1.0]{logo.pdf}\par}
        \centering
        \vspace*{2cm}
        {\Large Semestrální práce z KIV/PC\par}
        \vspace{1.5cm}
        {\Huge\bfseries JEDNODUCHÝ STEMMER\par}
        \vspace{2cm}

        {\Large Mukanova Zhanel\par}
        {\Large A16B0087P\par}
        {\Large mukanova@students.zcu.cz\par}

        \vfill

        {\Large 15.\,12.\,2018}
\end{titlepage}

\tableofcontents
\thispagestyle{empty}
\clearpage

\section{Zadání}
\setcounter{page}{1}
Naprogramujte v ANSI C přenositelnou konzolovou aplikaci, která bude pracovat jako tzv.
\textbf{stemmer}. Stemmer je algoritmus, resp. program, který hledá kořeny slov. Stemmer pracuje ve
dvou režimech: (i) v režimu učení, kdy je na vstupu velké množství textu (tzv. korpus) v jednom
konkrétním etnickém jazyce (libovolném) a na výstupu pak slovník (seznam) kořenů slov; nebo
(ii) v režimu zpracování slov, kdy je na vstupu slovo (nebo sekvence slov) a stemmer ke každému
z nich určí jeho kořen.

Stemmer se bude spouštět příkazem sistem.exe corpus-file | ["]word-sequence["]i [-msl=celé číslo] [-msf=celé číslo].
Symbol corpus-file zastupuje jméno vstupního textového souboru s korpusem, tj. velkým
množstvím textu, který se použije k natrénování stemmeru. Přípona souboru nemusí být uvedena;
pokud uvedena není, předpokládejte, že má soubor příponu .txt. Symbol word-sequence
zastupuje slovo nebo sekvenci slov, k nimž má stemmer určit kořeny. Režim činnosti programu
je dán předaným parametrem: Je-li parametrem jméno (a případní cesta k) souboru, pak bude
stemmer pracovat v režimu učení, tedy tvorby databáze kořenů na základě analýzy dat z korpusu.
Je-li parametrem slovo nebo sekvence slov (ta musí být uzavřena v uvozovkách), pak stemmer
bude pracovat v režimu zpracování slov, tedy určování kořene každého slova ze sekvence.
Program může být spuštěn se dvěma nepovinnými parametry: \\
\par -msl { Nepovinný parametr -msl=celé číslo specifikuje minimální délku kořene slova (msl =
Minimum Stem Length), který bude uložen do databáze kořenů. Není-li tento parametr
předán, použije se implicitní minimální délka kořene 3 znaky. Tento parametr je tedy zřejmě
použitelný jen v kombinaci s cestou ke korpusu, tedy v režimu učení stemmeru. \\
\par -msf { Nepovinný parametr -msf=celé číslo určuje minimální počet výskytů příslušného kořene
(msf = Minimum Stem Frequency). Pokud se tento kořen v korpusu nevyskytl alespoň
tolikrát, kolik je určeno tímto parametrem, nepoužije se při zpracování slov, tj. stemmer
nemůže u zádného zpracovávaného slova oznámit, že tento kořen je kořenem předmětného
slova. Není-li tento parametr předán, použije se implicitní minimální počet výskytů kořene
10×. Tento parametr je tedy zřejmě použitelný jen v kombinaci se slovem nebo sekvencí slov,
tedy v režimu zpracování slov. \\

Celé zadání je na: \emph{https://www.kiv.zcu.cz/studies/predmety/pc/doc/work/sw2018-03.pdf}	
\thispagestyle{empty}
\clearpage

\section{Analýza úlohy}

 Podle zadání jsme museli vytvořit program, který by byl schopen najít kořeny k zadaným slovům. Prvním problémem se kterým jsem se setkala byl správný výběr \textbf{datové struktury}. Prvním požadavkém bylo ukládaní seřazených slov podle ASCII hodnot. Druhým nejdůležitějším požadavkem byla rychlost vkládaní a získávání slov ze slovníku. Měla jsem několik variant datových struktur.

\subsection*{Hlavní datové struktury}

\begin{itemize}
\item \textbf{Hash-table}\\
Hashovací tabulka (hash table) je datová struktura, která slouží k ukládání dvojic klíč-hodnota. Tato tabulka kombinuje výhody vyhledávání pomocí indexu a procházení seznamu. Má nízké nároky na paměť a složitost \emph{O(1)}. \\
 Pokud bych chtěla pouze uložit slova a pak zkontrolovat, zda je mezi nimi hledané slovo nebo ne, pak by standardní hash tabulka byla rozumnou volbou. Pokud by počet položek seznamu byl znám předem, použila bych ideální hash pro dosažení nejvyššího výkonu a optimální velikosti uložených dat. \\
Hashovací tabulka je velmi efektivní vyhledávací metoda, avšak musíme vědět, kdy a jak ji použít. Musíme ještě zvážit alternativy jako třeba binární vyhledávací strom. Ten je v porovnání s hashovací tabulkou o něco pomalejší, pokud uvažujeme ideální případ, na druhou stranu pro případ nejhorší strom vítězí. Hashovací tabulka nám navíc neumožňuje vyhledávat klíče v určitém intervalu nebo jenom částečně specifikované klíče - např. všechny řetězce s určitou předponou.
\item \textbf{Prifix tree}\\
 Prefix tree nebo Trie je strom, který umožňuje uchovávat páry klíč - hodnota a získávat hodnoty podle klíče. Klíčem však musí být řetězec. Vyhledání dat v trie je rychlejší v nejhorším případě, tj. O(m), oproti hašovací tabulce s kolizemi O(N), typicky však O(1).\\
 Prefix tree v každém uzlu obsahuje všechny podřetězce, kterými může pokračovat řetězec v dosud prohledané cestě. Všechny následníci uzly mají společný prefix, který je shodný s řetězcem přiřazeným k danému uzlu. \\
 Prefix tree je vhodná datová struktura v případě rychlého vyhledávání předpony slov, i když to může být trochu neefektivní z hlediska velikosti uložených souborů. Tato datová struktura také podporuje rychlé vkládání a odstraňování. Navíc Prefix tree nepotřebuje hašovací funkce nebo její změna při přidávání dalších klíčů a ještě může poskytnout abecední řazení záznamů podle klíče, co v hash-tabulkách chybí.

\item \textbf{Dalsí stromy}\\
	 BK-Tree je skvělá datová struktura pro vytváření "slovníku" podobných slov, která použivá vzdáleností mezi slovy. Ta vzdálenost je minimální počet úprav potřebných k přeměně jednoho řetězce na druhý.
Kdybych chtěla použít slovník pro operace, jako je kontrola pravopisu, kde je potřebné najít slova, která nejsou podobná ostatním, je BK-strom skvělou volbou.
\end{itemize}

 Po dlouhém rozmýšlení jsem si zvolila Trie jako vhodnou datovou strukturu pro toto zadání. Zaprvé podle zadání jsem potřebovala uschovávat slova v abecedním pořadí, co neposkytuje Hash-tabulka. Zadruhé nepotřebuji vymýšlet Hash-funkce, a můžu rovnou ukládat slova do datové struktury bez vzniku kolizí.  \\
\par Další problém se kterým jsem se setkala bylo porovnání každého slova slovníku s ostatními. Kvůli tomu, že iterační funkce v datové struktuře Trie je obvyklé rekurzivní, musela jsem si zvolit nějakou \textbf{pomocnou datovou strukturu} pro procházení skrz celý slovník.

\subsection*{Pomocné datové struktury}

\begin{itemize}
  \item \textbf{Obecné pole retezcu}\\
  Pole je datová struktura, která sdružuje daný vždy konečný počet prvků stejného datového typu. K jednotlivým prvkům pole se přistupuje pomocí jejich indexu. Velikost pole zůstává při běhu programu neměnná a zvětšení pole je časově náročná operace. \\
 
  \item \textbf{Linked list}\\
    Spojový seznam (Linked list) je dynamická datová struktura, vzdáleně podobná poli (umožňuje uchovat velké množství hodnot ale jiným způsobem), obsahující jednu a více datových položek (struktur) stejného typu, které jsou navzájem lineárně provázány vzájemnými odkazy pomocí ukazatelů nebo referencí.
\end{itemize}

Hlávním úkolem této semestrální práce bylo najít kořeny slov. Existuje mnoho různých \textbf{algoritmu} nacházení kořenů. Já jsem potřebovala jen ty, co hledaly nejdelší společné podřetězce.

\subsection*{Algoritmy}

\begin{itemize}
  \item \textbf{Longest Common Subsequence (LCS)}\\
	LCS algoritmus je jedním ze způsobů jak posuzovat podobnost mezi dvěma řetězci. Ale neurčuje kořeny, určuje jenom “společná písmena” slov.
  \item \textbf{Longest Common Substring (LCS)}\\
	 Algoritmus zváží všechny podřetězce prvního slova a pro každý podřetězec zkontroluje, zda existuje podřetězec ve druhém slově. Vyhledá podřetězec maximální délky.
\end{itemize}

 Z těchto dvou algoritmu jsem si zvolila \textbf{Longest Common Substring (LCS)}, protože on nachází společné podřetězce, co jsem potřebovala podle zadání. Na jeho implementaci jsem potřebovala vytvořit 2d matici, do které bych mohla ukládat informace o společných podřetězců.

\thispagestyle{empty}
\clearpage

\section{Popis implementace} 

\subsection*{Obsah programu}

\begin{enumerate}

  \item \textbf{main.c + library.h} \\
 Hlavní spustitelný soubor programu, který obsahuje funkci \emph{main()}. Soubor má v sobě všechny funkce na parsování vstupních parametru a spuštění příslušných metod pro dva možných režimy programu (Režim učení a Režim zpracování slov). \\
  Hlavičkový soubor \textbf{library.h} obsahuje konstanty (IMPLICITNI\_MIN\_DELKA\_KORENE, IMPLICITNI\_MIN\_POCET\_VYSKYTU\_KORENE), určující implicitní minimální délky kořenů a počet jejich výskytů. Také konstanty (DICTIONARY, STEMS) cest k souborům \emph{stems.dat} a \emph{dictionary.txt}. \\ \\
Pomocné funkce.
	\begin{itemize}
	 \item \textbf{get\_parameter\_value()} - funkce pro získání celočíselného parametru. (např. -msl=9, získame 9)
     \item \textbf{match()} - funkce pro pracování s regulární výrazy
	\end{itemize}

Trénování stemmeru.
	\begin{itemize}
     \item \textbf{create\_dictionary(char *corpus\_file\_path)} - funkce přijímá jako vstupní parametr jméno vstupního textového souboru s korpusem, tj. velkým množstvím textu, který se použije k natrénování stemmeru, dále čté tento soubor a kážde slovo ukládá do datové struktury \emph{Trie}.
     \item \textbf{create\_words\_array(Word *root, char prefix[])} - rekurzívní funkce, která prochází přes celou datovou strukturu Trie a ukláda do LinkedListu seřezený slovník slov.
     \item \textbf{create\_stems\_file(int min\_delka\_korene)} - funkce vytváří nové Trie, pro kořeny slov. Pouští funkci \emph{compare\_strings()} a ukládá do souboru \emph{stems.dat} již seřazený podle ASCII hodnot slovník kořenů slov.
     \item \textbf{compare\_strings(FILE *fp, Trie* trie, List *head, int max\_delka\_slova)} - funkce dvakrát prochází přes LinkedList a pro káždé dva slovy pouští funkci \emph{LCS\_algorithm()}, která pomocí LCS algoritmu nachází nejdelší společný kořen a ukládá ho do datové struktury Trie.
     \end{itemize}
     
Režim zpracování slov.
     \begin{itemize}
     \item \textbf{create\_stems\_dictionary(const char *filename)} - funkce přijíimá jako vtupní parametr jméno souboru stems.dat a ukádá káždý kořen a jeho počet výkytů do datove truktury Trie.
     \item \textbf{find\_stems(char *word\_sequence, int msf\_value)} - funkce pro zádáné jako argument káždé slovo pouští funkci \emph{find\_longest\_word(char *str)} a výpisuje do konzoli společný nejdelší kořen
s dostatečným počtem výskytů.
   \end{itemize}
	
  \item \textbf{trie.c + trie.h} \\
  Soubory, které obsahují datovou strukturu Trie a všechné potřebné funkce jako vkládaní, procházení, inicializace, uvolnění a td. Také zahrnuje funkce jako \emph{str\_clean\_eng()} a \emph{str\_clean\_cz()}, které odstraňují z řetězce příslušného jazyka zbytečné znaky jako číslicí a td.\footnote{Je nutně měnit podle vstupního jazyka pro lepší výsledky}
  
  \begin{itemize}
  	\item \textbf{Trie* trie\_initialize()} - funkce pro inicializaci root node a allokování pamětí na datovou strukturu Trie.
	\item \textbf{Word* get\_new\_trie\_node()} - funkce vrátí Trie node s naallokovaným místem v pamětí. 
	\item \textbf{void insert(Trie* trie, char* str)} - vkládání slova do Trie
	\item \textbf{void display\_trie(FILE *fp, Word *root, char prefix[])} - zápis celého Trie do souboru.
    \item \textbf{void free\_t (Trie* trie)} - obalující funkce k funkci \emph{trie\_free()}. 
	\item \textbf{void trie\_free (Word *root)} - rekurzivní funkce pro uvolnění pamětí každého úzlu Trie.

	\item \textbf{int is\_leaf(Word* node)} - konroluje zda zadaný úzel je poslední (list).
	\item \textbf{void str\_clean\_cz (char* src)} - odstaňuje z řetězce všechné znaky z ASCII tabulky od 0 do 65. (funguje jen pro český text)
	\item \textbf{void str\_clean\_eng (char* src)} - pomocí standartní funkce \emph{isalpha()} nechává v řetězci jenom písmena.  (funguje jen pro anglický text text)
	\item \textbf{void find\_stem(Word* root, char *word, char prefix[], int msf\_value, char *MAX\_STR, size\_t stemSize)} - rekurzivní funkce, která prochází přes celý slovník kořenů a pomocí standartní funkci \emph{strstr()} nachází nejdelší společný kořen s dostatečným počtem výskytů.

	\item \textbf{char *find\_longest\_word (char *str)} - funkce najde v zadaném řetězci nejdelší slovo.
   \end{itemize}
  
  \item \textbf{lcs.c + lcs.h} \\
  Soubory zahrnující samotný algoritmus Longest Common Substring, který vyhledává nejdelší společný kořen, funkci inicializace a uvolnění matici pro výpočet nejdelšího společného kořenu.
  
   \begin{itemize}
		\item \textbf{int LCS\_algorithm(char *s1, char *s2, char **longest\_common\_substring)} - samotný algoritmus LCS (Longest Common Substring).
		\item \textbf{void init\_lcs\_matrix(int s1\_length, int s2\_length)} - inicializace matici pro LCS algoritmus.
		\item \textbf{void destroy\_lcs\_matrix ()} - uvolnění pamětí matici.
   \end{itemize}
  
  \item \textbf{linkedList.c + linkedList.h} \\
  Soubory obsahující implementace spojového seznámu, funkci pro vyhledávaní, vkládaní a procházení přes seznam.
  \begin{itemize}
		\item \textbf{List * make\_list()} - funkce výtváří datovou strukturu LinkedList, allokuje paměť a nastavuje root node.
		\item \textbf{Node * create\_node(char *data)} - funkce allokuje pamět pro úzel a vkládá data do úzlu.
		\item \textbf{void insert\_to\_list(char *data, List *list)} - funkce vkládá do LinkedListu nový úzel.
		\item \textbf{void destroy(List * list)} - uvolňuje celý LinkedList.
  \end{itemize}
  
\end{enumerate}

\subsection*{Postup}

Mým prvním krokem bylo ošetřit vstupní parametry. Nejdůležitější bylo zjistit podle parametrů co musí program dělat. Parametry \emph{-msl} a \emph{-msf }nejsou podle zadání povinné, a proto jsem nemohla zjistit podle těchto parametrů, zda musí program fungovat v režimu \textbf{učení} nebo \textbf{zpracování}. Na vyřešení tohoto problému jsem použila regulární výrazy, které by mohly zjistit zda je vstupní parametr cesta k souboru.

\subsection*{Režim učení}

 Po kontrole parametrů jsem začala implementovat datovou strukturu Trie. Vybrala jsem tuto strukturu pro její rychlé zpracování a uložení slov podle ASCII hodnot, což jsem potřebovala kvůli zadání. První problém se kterým jsem se setkala při implementaci Trie byl problém s kódováním. První můj Trie mohl ukládat do sebe jen 127 ASCII hodnot, a proto při vkládaní českých slov se program ukončil chybovým výpisem SIGNAL 11. Zvýšila jsem hodnotu ASCII symbolů z 127 do 256, což mě povolilo ukládat nejen anglické znaky, ale i české. Ale dostala jsem jiný problém. V hodnotě 256 nejsou jen písmena abeced, ale i různé znaky a číslice. To jsem vyřešila tím, že při vkládaní nového slova do Trie jsem kontrolovala jestli slovo neobsahuje ASCII hodnoty od 0 do 65, ostatní znaky jsem dělala nezápornými a dávala jsem je do nížního registru. \\
 Po vytvoření slovníku jsem musela porovnat každé slovo ve slovníku a vytvořit slovník kořenů. Ale procházet skrz celý Trie dvakrát by byla velmi náročná akce, protože ve svém programu jsem použila rekurzívní funkce procházení stromem. Takže jsem jedním procházením Trie vytvořila už seřazeny LinkedList pomocí kterého jsem porovnávala každá dva slova v slovníku. Na hledání kořenů jsem použila algoritmus LCS (longest common substring). Každý kořen jsem ukládala zase do nového Trie a potom už seřazeny slovník jsem ukládala do souboru \emph{stems.dat}.

\subsection*{Režim zpracování slov}

 Po spuštění programu v režimu zpracování slov, program nejprvé musí přečist už existující soubor \emph{stems.dat}, který obsahuje databázi kořenů. Během čtení suboru jsem ukládala každý kořen a počet jeho výskytů do datové struktury Trie. Dalším krokem bylo porovnat každé zadané slovo se slovníkem. Podle zadání jsem musela najít nejdelší společný kořen, který by měl nejvýsši ASCII hodnotu. Slova jsem porovnávala pomocí standardní funkce v C \emph{strstr()}, která by měla vrátit nenulovou hodnotu v případě jestli slovo obsahuje kořen. Pak jsem porovnávala velikost předchozího slova se slovem aktuálním a ukládala jsem do bufferu nejdelší společný kořen.

\section{Uživatelská příručka}

 Semestrální práce je vytvořena programovacím jazykem ANSI C a lze spustit různými způsoby. Hlavním požadavkem pro spuštění bude nainstalovaný kompilátor pro jazyk C \emph{GCC}.

\subsection*{Postup spuštění programu přes Linuxový Terminal}

\begin{enumerate}
\item Příkazem \emph{“cd path/to/program”} přijdeme do složky s programem.
\item Pro kompilace a generování spustitelného souboru použijeme příkaz \emph{“make”}, který vytvoří spustitelný soubor \emph{sistem.exe}
\item Pro spuštění programu v režimu učení použijeme příkaz  \emph{“./sistem.exe path/to/corpus/file  -msl=[cislo]”}. Parametr  \emph{”-msl=[cislo]” }není povinný. Určuje minimální délku kořene. Implicitní minimální délka kořene 3 znaky. Program vytvoří soubor stems.dat, který bude obsahovat kořeny slov.
\item Pro spuštění programu v režimu zpracování slov použijeme příkaz  \emph{“./sistem.exe “sekvence slov” -msf=[cislo]”}. Parametr  \emph{”-msf=[cislo]”} není povinny. Určuje minimální počet výskytů příslušného kořene. Program vypíše do Terminalu zadane slovo a  jeho koren.
\end{enumerate}

\subsection*{Postup spuštění programu v programovácím prostředí CLion}

\begin{enumerate}
\item Pro spuštění programu v režimu učení potřebujeme nejprve nastavit vstupné parametry. Ve složce Run->Edit Configurations..-> Program arguments dosadíme příkaz  \emph{“./sistem.exe path/to/corpus/file  -msl=[cislo]”}. Parametr  \emph{”-msl=[cislo]” }není povinný. Určuje minimální délku kořene. Implicitní minimální délka kořene 3 znaky. Program vytvoří soubor stems.dat, který bude obsahovat kořeny slov.
\item Pro spuštění programu v režimu zpracování slov nejprve nastavit vstupné parametry. Ve složce Run->Edit Configurations..-> Program arguments dosadíme příkaz  \emph{“./sistem.exe “sekvence slov” -msf=[cislo]”}. Parametr  \emph{”-msf=[cislo]”} není povinny. Určuje minimální počet výskytů příslušného kořene. Program vypíše do Terminálu zadané slovo a  jeho kořen.
\end{enumerate}

\section{Závěr}
 Dané zadání na začátku mně přišlo jednoduché. Ale během implementování jsem se dozvěděla, že pracování s řetězci v jazyce C je docela těžká akce.  Díky tomuto zadání jsem se seznámila s různými algoritmy a dočetla se mnoho o kódování. Tento úkol mi předal cenné zkušenosti v jazyce C. Kvůli obtížnosti zadání jsem musela všechno prostudovat a použít své znalosti ve svém programu. Myslím si, že zadání semestrální práce bylo splněno a tímto považuji semestrální práci za splněnou. Jediná výjimkou je, že aby program správně pracoval i s anglickým i s českým textem, musíme v programu v souboru Trie.c -> insert() změnit funkce \emph{str clean cz()} na \emph{str clean eng()}.
\end{document}
